{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97669,"databundleVersionId":11615683,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":1483651,"sourceType":"datasetVersion","datasetId":870709}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:04.970742Z","iopub.execute_input":"2025-05-07T12:06:04.971568Z","iopub.status.idle":"2025-05-07T12:06:04.975909Z","shell.execute_reply.started":"2025-05-07T12:06:04.971542Z","shell.execute_reply":"2025-05-07T12:06:04.974987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:04.977616Z","iopub.execute_input":"2025-05-07T12:06:04.977883Z","iopub.status.idle":"2025-05-07T12:06:04.993936Z","shell.execute_reply.started":"2025-05-07T12:06:04.977863Z","shell.execute_reply":"2025-05-07T12:06:04.992981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the train data\ntrain = pd.read_csv('/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:04.994906Z","iopub.execute_input":"2025-05-07T12:06:04.995262Z","iopub.status.idle":"2025-05-07T12:06:05.058381Z","shell.execute_reply.started":"2025-05-07T12:06:04.995233Z","shell.execute_reply":"2025-05-07T12:06:05.057437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:05.059512Z","iopub.execute_input":"2025-05-07T12:06:05.059898Z","iopub.status.idle":"2025-05-07T12:06:05.068522Z","shell.execute_reply.started":"2025-05-07T12:06:05.059868Z","shell.execute_reply":"2025-05-07T12:06:05.067745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The questions are LaTex formatted. They need to be coverted to plain text.","metadata":{}},{"cell_type":"code","source":"from pylatexenc.latex2text import LatexNodes2Text\n\n# Convert LaTeX-formatted strings in the 'Question' column to plain text using pylatexenc.\ntrain['Question_Text'] = train['Question'].apply(LatexNodes2Text().latex_to_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:05.128722Z","iopub.execute_input":"2025-05-07T12:06:05.129451Z","iopub.status.idle":"2025-05-07T12:06:21.466530Z","shell.execute_reply.started":"2025-05-07T12:06:05.129402Z","shell.execute_reply":"2025-05-07T12:06:21.465666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n\n# This function performs text preprocessing by: removing periods, tokenizing the text, converting tokens to lowercase, removing stopwords and lemmatizing tokens\n# The cleaned tokens are then rejoined into a single string.\n\ndef clean_text(text, remove_stopwords=True, lemmatize=True):\n    # Remove only the period character\n    text = re.sub(r'\\.', '', text)\n\n    # Tokenize\n    tokens = nltk.word_tokenize(text)\n\n    # Lowercase\n    tokens = [token.lower() for token in tokens]\n\n    # Remove stopwords\n    if remove_stopwords:\n        stop_words = set(stopwords.words('english'))\n        tokens = [token for token in tokens if token not in stop_words]\n\n    # Lemmatize\n    if lemmatize:\n        lemmatizer = WordNetLemmatizer()\n        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n\n    # Re-join tokens into a string\n    return ' '.join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:21.468188Z","iopub.execute_input":"2025-05-07T12:06:21.468485Z","iopub.status.idle":"2025-05-07T12:06:21.474562Z","shell.execute_reply.started":"2025-05-07T12:06:21.468462Z","shell.execute_reply":"2025-05-07T12:06:21.473726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['cleaned_question'] = train['Question_Text'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:21.475490Z","iopub.execute_input":"2025-05-07T12:06:21.475880Z","iopub.status.idle":"2025-05-07T12:06:26.788309Z","shell.execute_reply.started":"2025-05-07T12:06:21.475852Z","shell.execute_reply":"2025-05-07T12:06:26.787601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Convert cleaned text into TF-IDF feature vectors using unigrams, bigrams, and trigrams.\n# Limit the feature space to the top 100,000 terms based on term frequency across the corpus.\nvectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=100000)\nX = vectorizer.fit_transform(train['cleaned_question'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:26.789145Z","iopub.execute_input":"2025-05-07T12:06:26.789394Z","iopub.status.idle":"2025-05-07T12:06:28.351798Z","shell.execute_reply.started":"2025-05-07T12:06:26.789369Z","shell.execute_reply":"2025-05-07T12:06:28.350989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y = train.label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:28.354387Z","iopub.execute_input":"2025-05-07T12:06:28.354653Z","iopub.status.idle":"2025-05-07T12:06:28.359081Z","shell.execute_reply.started":"2025-05-07T12:06:28.354631Z","shell.execute_reply":"2025-05-07T12:06:28.358185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:28.359969Z","iopub.execute_input":"2025-05-07T12:06:28.360282Z","iopub.status.idle":"2025-05-07T12:06:28.376222Z","shell.execute_reply.started":"2025-05-07T12:06:28.360256Z","shell.execute_reply":"2025-05-07T12:06:28.375242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Optuna optimization search is designed to tune hyperparameters for a LightGBM model on a multiclass classification task. \n# It uses Stratified K-Fold cross-validation to for evaluation and aims to maximize the micro-averaged F1 score.\n\n'''import lightgbm as lgb\n\ndef objective_lgb(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1200),\n        'max_depth': trial.suggest_int('max_depth', 3, 14),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n        'subsample': trial.suggest_float('subsample', 0.65, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n        'objective': 'multiclass',\n        'verbosity': -1,\n        'random_state': 2,\n        'num_class': len(np.unique(y)),\n    }\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n    f1_scores = []\n\n    for train_idx, val_idx in skf.split(X, y):\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        model = lgb.LGBMClassifier(**params)\n        model.fit(X_train, y_train)\n\n        y_pred = model.predict(X_val)\n        score = f1_score(y_val, y_pred, average='micro')\n        f1_scores.append(score)\n\n    return np.mean(f1_scores)\n\nstudy_lgb = optuna.create_study(direction='maximize', study_name=\"lgbm_optuna\")\nstudy_lgb.optimize(objective_lgb, n_trials=100)\n\nprint(\"Best trial:\")\nprint(study_lgb.best_trial)'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:28.377446Z","iopub.execute_input":"2025-05-07T12:06:28.377793Z","iopub.status.idle":"2025-05-07T12:06:28.396340Z","shell.execute_reply.started":"2025-05-07T12:06:28.377761Z","shell.execute_reply":"2025-05-07T12:06:28.395381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Best hyperparameters obtained:\n\n{'n_estimators': 599, 'max_depth': 11, 'learning_rate': 0.032138984780093804, 'subsample': 0.7293348259364973, 'colsample_bytree': 0.7566540444923334, 'reg_lambda': 0.021337707070698972, 'reg_alpha': 0.06836150777087176}","metadata":{}},{"cell_type":"code","source":"# This Optuna optimization search tunes the smoothing parameter `alpha` for the Multinomial Naive Bayes model. \n# It uses Stratified K-Fold cross-validation for evaluation and aims to maximize the micro-averaged F1 score. \n# The goal is to find the best regularization strength to balance bias and variance in probabilistic text classification.\n\n'''from sklearn.naive_bayes import MultinomialNB\n\ndef objective_nb(trial):\n    alpha = trial.suggest_float('alpha', 1e-3, 10.0, log=True)\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n    f1_scores = []\n\n    for train_idx, val_idx in skf.split(X, y):\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        model = MultinomialNB(alpha=alpha)\n        model.fit(X_train, y_train)\n\n        y_pred = model.predict(X_val)\n        score = f1_score(y_val, y_pred, average='micro')\n        f1_scores.append(score)\n\n    return np.mean(f1_scores)\n\nstudy_nb = optuna.create_study(direction='maximize', study_name=\"nb_optuna\")\nstudy_nb.optimize(objective_nb, n_trials=50)\n\nprint(\"Best trial:\")\nprint(study_nb.best_trial)'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:28.421811Z","iopub.execute_input":"2025-05-07T12:06:28.422158Z","iopub.status.idle":"2025-05-07T12:06:28.445034Z","shell.execute_reply.started":"2025-05-07T12:06:28.422132Z","shell.execute_reply":"2025-05-07T12:06:28.444114Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Best alpha value:\n\nalpha = 0.061600028604369125","metadata":{}},{"cell_type":"code","source":"# This Optuna search tunes the regularization parameter `C` for the Linear Support Vector Classifier.\n# It uses Stratified K-Fold cross-validation for evaluation and aims to maximize the micro-averaged F1 score. \n\n'''from sklearn.svm import LinearSVC\n\ndef objective_svm(trial):\n    C = trial.suggest_float('C', 1e-3, 100.0, log=True)\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n    f1_scores = []\n\n    for train_idx, val_idx in skf.split(X, y):\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        model = LinearSVC(C=C, max_iter=10000, random_state=2)\n        model.fit(X_train, y_train)\n\n        y_pred = model.predict(X_val)\n        score = f1_score(y_val, y_pred, average='micro')\n        f1_scores.append(score)\n\n    return np.mean(f1_scores)\n\nstudy_svm = optuna.create_study(direction='maximize', study_name=\"svm_optuna\")\nstudy_svm.optimize(objective_svm, n_trials=50)\n\nprint(\"Best trial:\")\nprint(study_svm.best_trial)'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:06:28.446018Z","iopub.execute_input":"2025-05-07T12:06:28.446320Z","iopub.status.idle":"2025-05-07T12:06:28.466864Z","shell.execute_reply.started":"2025-05-07T12:06:28.446291Z","shell.execute_reply":"2025-05-07T12:06:28.466046Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Best C value:\n\nC = 14.534588753905727","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\n\n# Define a LightGBM model with the best hyperparameters obtained.\nparams={'n_estimators': 599, 'max_depth': 11, 'learning_rate': 0.032138984780093804, 'subsample': 0.7293348259364973, 'colsample_bytree': 0.7566540444923334, 'reg_lambda': 0.021337707070698972, 'reg_alpha': 0.06836150777087176, 'verbosity': -1}\nlgbm = lgb.LGBMClassifier(**params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:17:16.384865Z","iopub.execute_input":"2025-05-07T12:17:16.385263Z","iopub.status.idle":"2025-05-07T12:17:20.759980Z","shell.execute_reply.started":"2025-05-07T12:17:16.385239Z","shell.execute_reply":"2025-05-07T12:17:20.759090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\n# Define a Linear Support Vector Classifier with the best C.\nC = 14.534588753905727\nlsvc = LinearSVC(C=C, max_iter=10000, random_state=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:17:20.761629Z","iopub.execute_input":"2025-05-07T12:17:20.762370Z","iopub.status.idle":"2025-05-07T12:17:20.766878Z","shell.execute_reply.started":"2025-05-07T12:17:20.762345Z","shell.execute_reply":"2025-05-07T12:17:20.766003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\n# Define a Multinomial Naive Bayes model with the best alpha.\nalpha = 0.061600028604369125\nmnb = MultinomialNB(alpha=alpha)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:17:20.767855Z","iopub.execute_input":"2025-05-07T12:17:20.768159Z","iopub.status.idle":"2025-05-07T12:17:20.794140Z","shell.execute_reply.started":"2025-05-07T12:17:20.768137Z","shell.execute_reply":"2025-05-07T12:17:20.793143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# Create an ensemble model using the classifiers defined.\nmodel = VotingClassifier(\n    estimators=[\n        ('lgbm', lgbm),\n        ('svm', lsvc),\n        ('mnb', mnb)\n    ],\n)\n\n# Fit on training data\nmodel.fit(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:17:34.225585Z","iopub.execute_input":"2025-05-07T12:17:34.226672Z","iopub.status.idle":"2025-05-07T12:18:19.402418Z","shell.execute_reply.started":"2025-05-07T12:17:34.226641Z","shell.execute_reply":"2025-05-07T12:18:19.401506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the test data\ntest = pd.read_csv('/kaggle/input/classification-of-math-problems-by-kasut-academy/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:19:32.559570Z","iopub.execute_input":"2025-05-07T12:19:32.559964Z","iopub.status.idle":"2025-05-07T12:19:32.586047Z","shell.execute_reply.started":"2025-05-07T12:19:32.559929Z","shell.execute_reply":"2025-05-07T12:19:32.585017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert LaTeX-formatted questions in the test set to plain text \n# then clean the resulting text by applying the same preprocessing as done for the training data.\ntest['Question_Text'] = test['Question'].apply(LatexNodes2Text().latex_to_text)\ntest['cleaned_question'] = test['Question_Text'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:19:33.881016Z","iopub.execute_input":"2025-05-07T12:19:33.882036Z","iopub.status.idle":"2025-05-07T12:19:38.919182Z","shell.execute_reply.started":"2025-05-07T12:19:33.881988Z","shell.execute_reply":"2025-05-07T12:19:38.918308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transform the cleaned test questions into TF-IDF feature vectors using the fitted vectorizer\nX_test = vectorizer.transform(test['cleaned_question'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:19:40.514756Z","iopub.execute_input":"2025-05-07T12:19:40.515144Z","iopub.status.idle":"2025-05-07T12:19:40.699932Z","shell.execute_reply.started":"2025-05-07T12:19:40.515121Z","shell.execute_reply":"2025-05-07T12:19:40.698992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict the labels for the test set using the trained ensemble model\npreds = model.predict(X_test)\ntest['label'] = preds\n\n# Save the 'id' and predicted 'label' columns to a CSV file for submission\ntest[['id','label']].to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:19:40.700876Z","iopub.execute_input":"2025-05-07T12:19:40.701178Z","iopub.status.idle":"2025-05-07T12:19:41.464904Z","shell.execute_reply.started":"2025-05-07T12:19:40.701157Z","shell.execute_reply":"2025-05-07T12:19:41.464041Z"}},"outputs":[],"execution_count":null}]}